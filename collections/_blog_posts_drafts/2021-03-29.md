---
layout: post
title:  The Idea Machine 
tags: idea-machine 2020 MIT Harvard
---

## Learning at Harvard and MIT in the Age of Artificial Intelligence

### Numerical Issues with Mixtures of Gaussians

My memory engram model relies on performing expectation maximization on a mixture of 
Gaussians. The M-step of EM in particular requires performing gradient updates
of the Gaussian parameters. Despite having seen this model repeatedly, I never 
realized how numerically unstable EM is. In the past two days, I've run into several
issues. The first problem arose from performing gradient ascent on the covariance. Computing the
the gradient of the log likelihood with respect to the covariance seems straightforward:

$$\begin{align*}
\nabla_{\Sigma}\log p(x; \mu, \Sigma)
&= \nabla_{\Sigma} \Big( -0.5 \log |\Sigma| - 0.5 (x- \mu)^T \Sigma^{-1} (x - \mu) \Big)\\
&= 0.5 * \Sigma^{-1} - 0. 5 \Sigma^{-1} (x - \mu)(x- \mu)^T \Sigma^{-1} 
\end{align*}$$

However, remember that covariances for Normal distributions must be positive definite.
There's nothing here constraining the gradient step to respect the positive definiteness
of the covariance matrix, and I found that my covariances would quickly become singular
under gradient descent. The solution is to reparameterize the covariance as

$$ \Sigma = A^T A$$

and optimize $$A$$ instead. The second problem arose from the gradient update for the mean.
If some dimensions have near zero variance, the gradient update will blow up:

$$\nabla_{\mu} \log p(x; \mu, \Sigma) = \Sigma^{-1}(x-\mu)$$

So the covariance really needs to be defined as 

$$ \Sigma = c I + A^T A$$

I thought my code had bugs and spent 3-4 hours trying to debug before asking a friend, who 
immediately pointed me towards [Unconstrained Parameterizations 
for Variance-Covariance Matrices](https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.31.494&rep=rep1&type=pdf)

### Memory Engrams

I continued my blitz of reading memory engram papers. Today I read [Liu Nature 2012](
../kernel_papers/liu_nature_2012_optogenetic_stimulation.html) 
and [Ramirez Science 2013](kernel_papers/ramirez_science_2013_false_memory.html).
Both were early papers in 

