---
layout: post
author: Sanh, Webson, Raffel, Bach et al. (Arxiv 2021)
title: Multitask Prompted Trainig Enables Zero-Shot Task Generalization
date: 2021-10-31
tags: natural-language-processing transfomers hugging-face
---

## Background

Large Language Models (LLMs) attain reasonable zero-shot generalization on
diverse set of tasksl; I disagree - I think [GPT3](https://arxiv.org/pdf/2005.14165.pdf)
is not very good - but these authors cite GPT-3 regardless.

Radford 2019 hypothesized generalization is result of implicit multi-task learning
i.e. next token prediction forces language model to learn from mixture of 
implicit tasks.

## Research Questions

## Experiment 1

### Setup 1

