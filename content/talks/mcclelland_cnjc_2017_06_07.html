<!DOCTYPE html>
<html>

  <head>
    <meta charset='utf-8'>
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <meta name="viewport" content="width=device-width,maximum-scale=2">
    <meta name="description" content="Rylan Schaeffer">

    <link rel="stylesheet" type="text/css" media="all" href="../../stylesheets/stylesheet.css">

    <title>McClelland's Stanford's CNJC Talk</title>

    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-94033137-1', 'auto');
      ga('send', 'pageview');
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
    </script>

    <script type="text/javascript"
      src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({ TeX: { equationNumbers: {autoNumber: "AMS"} } });
    </script>

    $\DeclareMathOperator*{\argmax}{argmax}$
  </head>

  <body>

    <!-- HEADER -->
    <div id="header_wrap" class="outer">
      <header class="inner">
        <h1 id="project_title">Speaker Series<br>McClelland at Stanford's Computational Neuroscience Journal Club</h1>
        <h3 id="project_tagline"></h3>
          <section id="quick_links">
            <a class="back" href="../research.html"></a>
          </section>
      </header>
    </div>

    <!-- MAIN CONTENT -->
    <div id="main_content_wrap" class="outer">
      <section id="main_content" class="inner">
        <p>
          One of the privileges and pleasures that living in Silicon Valley affords is the opportunity to attend fascinating talks by giants in academia and industry alike. I'm cognizant of the fact that there are some who may wish to attend, but are unable to do so, and so I decided to share my notes digitally and freely.
        </p><br>

        <p>Disclaimer: the following notes capture the talk as I perceived it and do not necessarily accurately capture the speaker's point as intended.</p><br>


        <h2>Speaker</h2>
        <p>
          Professor <a href="https://en.wikipedia.org/wiki/James_McClelland_(psychologist)">James McClelland</a> is a professor of psychology at Stanford University, perhaps best known for his co-authorship of <a href="https://mitpress.mit.edu/books/parallel-distributed-processing">Parallel Distributed Processing</a>, a book which sparked a resurgence in <a href="https://en.wikipedia.org/wiki/Connectionism">connectionism</a> and neural-network based modeling. The ambiguously attributed third author, the "PDP Research Group," included luminaries such as <a href="https://en.wikipedia.org/wiki/Geoffrey_Hinton">Geoffrey Hinton</a>, <a href="https://en.wikipedia.org/wiki/Francis_Crick">Francis Crick</a>, <a href="https://en.wikipedia.org/wiki/Michael_I._Jordan">Michael Jordan</a> and <a href="https://books.google.com/books?id=nQMPIGd4baQC&pg=PA946&lpg=PA946&dq=PDP+research+group+uc+san+diego&source=bl&ots=GgEYPDqlxz&sig=79QRzbZtGwybGVVewXWssdQkM9k&hl=en&sa=X&ved=0ahUKEwjI4vXRh7XUAhWhzIMKHcI5DVkQ6AEIRDAF#v=onepage&q&f=false">many more</a>.
        </p><br>

        <h2>Hosting Organization</h2>
        <p>
          Stanford's <a href="https://web.stanford.edu/group/mbc/JournalClub/">Computational Neuroscience Journal Club (CNJC)</a> is a cross-departmental, biweekly meeting group that discusses recent papers in and related to computational neuroscience. The club is organized by <a href="http://web.stanford.edu/~smvyas/">Saurabh Vyas</a> and <a href="http://alexhwilliams.info/">Alex Williams</a>.
        </p>

        <h2>Slides</h2>

        <object data="mcclelland_cnjc_2017_06_07.pdf" width="100%" height="500px" type='application/pdf'></object>

        <h2>Content</h2>

        <h3>Late 1950s</h3>
        <p>
          <ul>
            <li>Early approaches to artificial intelligence diverged</li>
            <li><a href="https://en.wikipedia.org/wiki/Frank_Rosenblatt">Rosenblatt</a> (creator of the delta rule) worked on perceptron (statistical)</li>
            <li><a href="https://en.wikipedia.org/wiki/Noam_Chomsky">Chomsky</a> worked on syntactic structure (logical)</li>
            <li>extreme contrast between the two</li>
            <li><a href="https://en.wikipedia.org/wiki/Herbert_A._Simon">Simon</a> and <a href="https://en.wikipedia.org/wiki/Allen_Newell">Newell</a> claimed to have created artificial intelligence with their <a href="https://en.wikipedia.org/wiki/General_Problem_Solver">General Problem Solver</a>, but really all it did was transform logical propositions into other logical propositions</li>
          </ul>
        </p>

        <h3>1970s</h3>
        <p>
          <ul>
            <li>McClelland did undergrad (in something related to animal conditioning?)</li>
            <li>McClelland remembers a quote in the 1967 default undergraduate psychology textbook by Neisser: the discoveries made by cognitive psychologists were "only of peripheral interest" towards understanding intelligence</li>
            <li>Learned that phenomenon could be explained through interactions of neurons e.g. horseshoe crab neurons fired in response to light stimuli</li>
            <li>Researchers moved away from artificial neural networks because they were hugely computationally expensive</li>
            <li>Two notables continued working on neural networks: <a href="https://en.wikipedia.org/wiki/Stephen_Grossberg">Grossberg</a> and <a href="https://en.wikipedia.org/wiki/James_A._Anderson">Anderson</a></li>
            <li>Grossberg was the original Schmidhuber: made numerous contributions, took credit for many, many more</li>
            <li>Grossberg showed how competitive pools of neurons could capture many perceptual phenomena. He also introduced competitive learning</li>
            <li>Anderson used matrix/vector-based approach to modeling (which we take for granted today)</li>
            <li>Anderson worked on attractor dynamics. He would create models of 50-100 neurons and demonstrated attractors as certain inputs would get sucked into a corner of the hypercube</li>
          </ul>
        </p>

        <h3>Late 1970s</h3>
        <p>
          <ul>
            <li>Enter <a href="https://en.wikipedia.org/wiki/David_Rumelhart">Rumelhart</a>. The guy is a wicked smart. McClelland felt intimidated. The guy had created his own version of LISP because he disliked some of its features.</li>
            <li>Rumelhart was dissatisfied with symbolic AI. He felt that perception/comprehension required graded constraint satisfaction (what is this?), which wasn't easily captured with LISP-like processing</li>
            <li>Geoffrey Hinton somehow entered the picture. He came from studying holograms - specificially, pattern completion with neural networks</li>
            <li>Hinton had some project of extracting rudimentary semantics(?)</li>
          </ul>
        </p>

        <h3>Early 1980s</h3>
        <p>
          <ul>
            <li>Group gathered in UCSD: Francis Crick, Rumelhart, McClelland, Michael Jordan, Hinton</li>
            <li>Rumelhart and McClelland wrote PDP book while Hinton and <a href="https://en.wikipedia.org/wiki/Terry_Sejnowski">Sejnowski</a> worked on Boltzmann Machines and Rumelhart, Hinton and <a href="https://en.wikipedia.org/wiki/Ronald_J._Williams">Williams</a> worked on adapting backpropagation to neural networks</li>
            <li>Tension when writing the book because Crick wanted to stick to the data and backprop is biologically implausible (synapses are directed)</li>
          </ul>
        </p>

        <h3>Are Rules a Thing of the Past?</h3>
        <p>
          <ul>
            <li>This was a random aside. I don't know how it fit into the talk at this point</li>
            <li>Psychologists had observed children creating new past-tense conjugations of verbs e.g. "taked" instead of "took"</li>
            <li>Someone (McClelland?) tried to teach neural networks to learn past tense congujations i.e. given current tense, output past tense</li>
            <li>trained neural network with regular verbs, then irregular verbs</li>
            <li>Network made similar mistakes as children while training, and similarly overcame those mistakes as training went on</li>
            <li>Network could successfully pattern-recognize to unseen inputs. For example, network could output "weep->wept" based on input "keep->kept"</li>
          </ul>
        </p>

        <h3>Mid 1980s</h3>
        <p>
          <ul>
            <li>Pinker and Fodor invited McClelland to a debate at MIT on theories as laid out in PDP</li>
            <li>Pinker and Fodor got to give back-to-back opening remarks (an hour each), then gave the audience a break, at which point everyone left</li>
          </ul>
        </p>

        <h3>Backpropagation</h3>
          <p>
            <ul>
              <li>Rumelhart was interested in negation problem i.e. 1 layer couldn't negate input</li>
              <li>Rumelhart suggested hidden layer to side that could override input</li>
              <li>Neurobiologists rejected backprop as umplausible</li>
              <li>Artificial neural networks led some neurobiologists to a crisis of faith. Many had hoped for desireable computation, but while neural networks were extremely deterministic, they were almost impossible to interpret</li>
            </ul>
          </p>

        <h3>Cognitive Neuropsychology</h3>
          <p>
            <ul>
              <li>In 1970s, some researchers decided to use artificial neural networks to model brain damage</li>
              <li>Deep dyslexics can't read non-words e.g. Vint. Shallow dyslexics make different errors.</li>
              <li>Modeled brain behavior with three neural networks (semantics, orthography, phonology), with connections between pairs.</li>
              <li>Lesioning specific pathways resulted in observed problems in patients, demonstrating that brain damage could be modeled</li>
            </ul>
          </p>

        <h3>Late 90s</h3>
          <p>
            <ul>
              <li>Neural networks entered research winter</li>
              <li>NN couldn't solve problems that people claimed they could e.g. visual object recognition</li>
              <li>Depth was viewed as an enemy, as depth frequently slowed down learning multiplicatively</li>
              <li>Bayesian approaches became popular. By early 2000s, Stanford had stopped teaching neural networks.</li>
            </ul>
          </p>

        <h3>New Directions</h3>
          <p>
            <ul>
              <li>Thinks promising research lies in the direction of experience-driven development of language, motor skills and cognitive systems</li>
              <li>This will require datasets like those available to children</li>
              <li>Specifically says he's most impressed with the work of <a href="http://contrastiveconvergence.net/~timothylillicrap/index.php">Tim Lillicrap</a>, <a href="https://en.wikipedia.org/wiki/Alex_Graves_(computer_scientist)">Alex Graves</a> and Greg Wayne (can't find a webpage)</li>
            </ul>
          </p>

        <h2>Notes</h2>
        <p>
          I appreciate any and all feedback. If I've made an error or if you have a suggestion, you can <a href="mailto:ryschaeffer@ucdavis.edu">email me</a> or comment on the <a href="">Reddit</a> thread.
        </p>

      </section>
    </div>

    <!-- FOOTER  -->
    <div id="footer_wrap" class="outer">
      <footer class="inner">
        
        <p class="copyright">Slate theme maintained by <a href="https://github.com/pages-themes">pages-themes</a></p>
        
        <p>Published with <a href="https://pages.github.com">GitHub Pages</a></p>
      </footer>
    </div>

    
  </body>
</html>
