<!DOCTYPE html>
<html lang="en-US">

  <head>
    <meta charset='utf-8'>
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <meta name="viewport" content="width=device-width,maximum-scale=2">
    <meta name="description" content="Rylan Schaeffer">

    <link rel="stylesheet" type="text/css" media="all" href="../../stylesheets/stylesheet.css">

    <title></title>

    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-94033137-1', 'auto');
      ga('send', 'pageview');
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
    </script>

    <script type="text/javascript"
      src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({ TeX: { equationNumbers: {autoNumber: "AMS"} } });
    </script>

    <div style="display: none">
      $\DeclareMathOperator*{\argmax}{argmax}$
      $\DeclareMathOperator*{\argmin}{argmin}$
      $\DeclareMathOperator{\defeq}{\stackrel{def}{=}}$
      $\DeclareMathOperator{\Tr}{Tr}$
      $\DeclareMathOperator{\rank}{rank}$
      $\DeclareMathOperator{\sign}{sign}$
    </div>
  </head>

  <body>

    <!-- HEADER -->
    <div id="header_wrap" class="outer">
      <header class="inner">
        <h1 id="header_title">Rylan Schaeffer > Learning > Neural Networks</h1>
        <nav>
          <div>
            <a href="../../index.html">Home</a> |
            <a href="../learning.html">Learning</a>
          </div>
        </nav>
      </header>
    </div>

    <!-- MAIN CONTENT -->
    <div id="main_content_wrap" class="outer">
      <section id="main_content" class="inner">

        <div id="toc_container">
          <h1 class="toc_title">Contents</h1>
          <ul class="table-of-content" id="markdown-toc">
            <li><a href="#learning_algorithms">Learning Algorithms</a>
              <ul>
                <li><a href="#backprop">Backpropagation</a></li>
                <li><a href="#weight_perturbation">Weight Perturbation</a></li>
                <li><a href="#node_perturbation">Node Perturbation</a></li>
                <li><a href="#bp_vs_wp_vs_np">Comparison of Backprop, Weight Perturbation & Node Perturbation</a></li>
              </ul>
            </li>

            <li><a href="#linear_models">Linear Models</a>
              <ul>
                <li><a href="#scalar_linear_neuron">Scalar Linear Neuron</a></li>
                <li><a href="#pca_networks">PCA Networks</a></li>
                <li><a href="#deep_linear_nn">Deep Linear Neural Networks</a></li>
              </ul>
            </li>

            <li><a href="#nonlinear_models">Nonlinear Neural Networks</a>
                <ul>
                    <li><a href="#perceptron">Perceptron</a></li>
                </ul>
            </li>

            <li><a href="#activation_functions">Activation Functions</a>
              <ul>
                <li><a href="#logistic_sigmoid">Logistic Sigmoid </a></li>
                <li><a href="#hyperbolic_tangent">Hyperbolic Tangent</a></li>
                <li><a href="#relationship_sigmoid_tanh">Relationship between
                  Logistic Sigmoid and Hyperbolic Tangent</a></li>
              </ul>
            </li>
          </ul>
        </div>

        <h2 id="linear_models">Linear Models</h2>
        <div>
          <h3 id="scalar_linear_neuron">Scalar Linear Neuron</h3>
          <div>

            <p>
              One of the simplest models of a neuron we might consider is a single "neuron" outputting
              a weighted combination of input features:
            </p>

            <p>
              $$
              \begin{align*}
              y = w^T x
              \end{align*}
              $$
            </p>

            <p>
              where $y \in \mathbb{R}$ and $w, x \in \mathbb{R}^d$.  This model, as a supervised learning
              problem under the Mean-Squared Error loss function, has been extensively studied. In such a setting, the
              neuron's goal is to learn a mapping $x \rightarrow y$ from a dataset of N x-y pairs,
              $\{(x_n, y_n) \}_{n=1}^N$, by minimizing the training MSE $L^{tr}(w) \defeq \frac{1}{N} \sum_{n=1}^N
              (y_n - w^T x_n)^2$. Does this model have a unique fixed point $w^*$ that minimizes the
              loss? We take the gradient and set equal to zero.
            </p>

            <p>
              $$
              \begin{align*}
              L^{tr}(w) &\defeq \frac{1}{N} \sum_{n=1}^N (y_n - w^T x_n)^2 = \sum_{n=1}^N Tr(y_n - w^T x_n)^2\\\\
              \nabla_w L(w) = 0 &= \nabla_w \frac{1}{N} \sum_{n=1}^N (y_n^2 - 2 w^T x_n y_n + w^T x_n x_n^T w)\\
              0 &= -\sum_{n=1}^N y_n x_n + \sum_{n=1}^N x_n x_n^T w^*\\
              \sum_{n=1}^N x_n x_n^T w^* &= \sum_{n=1}^N y_n x_n
              \end{align*}
              $$
            </p>

            <p>
                One important observation in that $\sum_{n=1}^N x_n x_n^T$ is the empiric input-input
                covariance matrix and $\sum_{n=1}^N  y_n x_n$ is the empiric output-input covariance
                "matrix" (although here the covariance is a vector since $y$ is a scalar), and the
                optimal weight vector $w^*$ relates these two covariance matrices. Another important
                observation is that $C \defeq \sum_{n=1}^N x_n x_n^T$ is the sum of $N$ rank-$D$
                matrices, meaning that $\rank(C) \leq min(N, D)$. The third important observation is that the
                Hessian $H_{ij} \defeq \frac{\partial L^{tr}(w)}{\partial w_i \partial w_j}= C_{ij}$.
                More on this in a bit.
            </p>

            <p>
                The first question to ask is what behavior we should expect from such a model?
                Two regimes emerge, one where $N < D$ and the other where $N \geq D$. In the first regime,
                where the number of observations $N$ is less than the dimensionality of the data $D$, $C$
                is a $D \times D$ matrix of at most rank $N$, meaning that $C$ is not invertible and
                therefore infinitely many solution exist. The easiest way to see this
                is to note that we have $N$ equations and $D$ unknowns, meaning we have fewer equations
                than unknowns. Consequently, in this regime, the training error $L^{tr}(w)$ is guaranteed to be 0.
                For those unconvinced, we can prove this claim via construction:
            </p>

            <p>
                <!--          The second observation is that $w$ is a minimum because the Hessian of the loss function-->
                <!--          $H_{ij} = \frac{\partial L(w)}{\partial w_i \partial w_j} = \sum_{n=1}^N x_n x_n^T$.-->
                <!--          Because <a href="linear_algebra.html#covariance_pos_semidefinite">covariance matrices are always-->
                <!--          positive-semidefinite</a>, the input-input covariance eigenvalues are $\geq 0$-->
            </p>
            </div>

          <h3 id="pca_networks">PCA Networks</h3>
          <div>
            <p>


              Previously, we considered the linear neuron learning via supervised labels $y$. Here,
              we'll instead consider a linear neuron receiving data $x \in \mathbb{R}^d$ and learning
              a linear readout that produces a low dimension representation $y \in \mathbb{R}^k$, where
              $k < d$:

              $$y = w^T x $$

              Rather than defining an objective function and then showing the emergent update
              rule, we'll instead consider an update rule inspired by Hebbian learning principles: if
              the model's output is correlated with a feature in the input space, the corresponding
              weight should be strengthened. This learning rule will be shown to a common linear
              unsupervised learning technique, principal component analysis on the data. The Hebbian
              update rule is:

              $$w_{t+1} = w_{t} + \eta y_t x_t $$
            </p>

            <p>
              However, this naive Hebbian learning rule is unstable! We can see this by considering
              the continuous time differential equation and showing that the magnitude of $w$ diverges.

              $$w_{t+1} - w_t = \eta y_t x_t \approx \frac{\Delta w}{\Delta t} = \frac{\eta}{\Delta t} y_t x_t
              \approx \tau \frac{dw}{dt} = y(t) x(t) $$

              But if we replace $y_t = w_t^T x_t$, we see the the change in $w$ scales proportional
              to the norm of $x(t)$:

              $$
              \begin{align*}
              \tau \frac{dw}{dt} &= y(t) x(t) \\
              &= w(t)^T x(t) x(t)\\
              &= x(t) w(t)^T x(t) \\
              \tau \frac{d}{dt} ||w_t||_2^2 &= w(t)^T x(t) w(t)^T x(t)\\
              &= ||w(t) x(t)||_2^2
              \end{align*}
              $$

              This implies that $w(t)$ will diverge to infinity since the change is always positive,
              regardless of inputs. We can also show the direction that $w$ diverges in by
              projecting the weight vector onto the data covariance matrix $C = x(t) x(t)^T$:

              $$
              \begin{align*}
              \tau \frac{dw}{dt} &= x(t) x(t)^T w(t)\\
              &= C w(t)\\
              &= C \sum_i w_i(t) \vec{\lambda}_i\\
              \tau \frac{dw_i}{dt} &= \lambda_i w_i(t)
              \end{align*}
              $$

              Solving the dynamics yields
            </p>

            <p>
              To fix this, we propose a slight modification of the Hebbian learning rule called
              Oja's rule:


            </p>
          </div>

          <h3 id="ojas_rule">Oja's Rule</h3>
          <div>

          </div>


          <h3 id="sangers_rule">Sanger's Rule</h3>

          <h3 id="deep_linear_nn">Deep Linear Neural Networks</h3>
          <div>
            <p>
              A deep linear neural network is a multi-layered neural network with the non-linear
              "activations" removed. That is, it is a composition of linear
              transformations (typically matrices). For instance,
              $\hat{y} = W_{32} W_{21} x$ is a two-layer linear network. Despite
              lacking the ability to learn non-linear functions, this class of
              network is attractive because it can be tractably analyzed and
              because its behaves similarly to its non-linear cousins under mild
              conditions.
            </p>

            <p>
              One question we might immediately ask is <b>for a two-layer linear neural network trained
              under mean squared
                error (MSE) on dataset $\{(x_i, y_i)\}_1^N$, what are the coupled differential
                equations that describe the network's learning dynamics?</b>
              Let $x \in \mathbb{R}^i$ be the input, $y \in \mathbb{R}^o$ be the
              output, and $L = \frac{1}{N} \sum_{i=1}^N (y - W_{32} W_{21} x)^T (y - W_{32} W_{21} x)$
              be the objective function. Define the input-input correlation matrix
              $\Sigma_{11} = \frac{1}{N}\sum_{i=1}^N x x^T$ and the input-output
              correlation matrix $\Sigma_{31} = \frac{1}{N}\sum_{i=1}^N y x^T$. We first
              derive $-\frac{\partial L}{\partial W_{32}}$:
            </p>

            <p>
              $$
              \begin{align*}
              -\frac{\partial L}{\partial W_{32}} &= -\frac{1}{\partial W_{32}} \frac{1}{N} \sum_{i=1}^N (y - W_{32} W_{21} x)^T (y - W_{32} W_{21} x) \nonumber \\
              &= -\frac{1}{\partial W_{32}} \frac{1}{N} \sum_{i=1}^N tr(y^T y - 2 y^T W_{32} W_{21} x + x^T W_{21}^T W_{32}^T W_{32} W_{21} x) \nonumber \\
              &= \frac{1}{N} \sum_{i=1}^N 2 \frac{1}{\partial W_{32}} tr(y^T W_{32} W_{21} x) - \frac{1}{\partial W_{32}}tr(x^T W_{21}^T W_{32}^T W_{32} W_{21} x) \nonumber \\
              &= \frac{1}{N} \sum_{i=1}^N 2 y x^T W_{21}^T - 2 W_{32} W_{21} x x^T W_{21}^T \nonumber \\
              &= 2 \Sigma_{31} W_{21}^T - 2 W_{32} W_{21} \Sigma_{11} W_{21}^T \nonumber \\
              -\frac{\partial L}{\partial W_{32}} &= 2 (\Sigma_{31} - W_{32} W_{21} \Sigma_{11}) W_{21}^T
              \end{align*}
              $$
            </p>

            <p>
              We similarly derive $-\frac{\partial L}{\partial W_{21}}$:
              $$
              \begin{align*}
              -\frac{\partial L}{\partial W_{21}} = 2 \Sigma_{32}^T (\Sigma_{31} - W_{32} W_{21} \Sigma_{11})
              \end{align*}
              $$
            </p>

            <p>
              Together, these coupled nonlinear differential equations define the
              learning dynamics of a linear neural network. One key aspect is that
              the learning dynamics are non-linear, meaning that deep linear networks
              are more sophisticated than shallow (i.e. single-layer) networks. These
              non-linear learning dynamics arise from the choice of error function:
              MSE produces a loss function that is quartic with respect to the weights,
              resulting in a gradient that is cubic with respect to the weights.
            </p>
            </div>
          </div>

        <h2 id="nonlinear_models">Nonlinear Models</h2>
        <div>
              <h3 id="perceptron">Perceptron</h3>
              <div>
                  <p>
                      One of the earliest and simplest models of a nonlinear neural network is
                      a binary classifier called the Perceptron, the non-linear equivalent of the
                      <a href="#scalar_linear_neuron"> single linear neuron</a>:
                  </p>

                  <p>
                      $\begin{align*}
                      y = \sign(w^T x)
                      \end{align*}$
                  </p>

                  <p>
                      An amazing property of the perceptron was that for a given classification
                      problem, if the
                  </p>

              </div>
            </div>



      </section>
    </div>

    <!--FOOTER  -->
    <div id="footer_wrap" class="outer">
      <footer class="inner">
        <h3 id="footer_title">Rylan Schaeffer</h3>
        <p>
          I appreciate any and all feedback. If you have suggestions or feedback or want to get in touch,
          ping me at <a href="mailto:rylanschaeffer@gmail.com">rylanschaeffer@gmail.com</a>.

          If you feel like helping me survive grad school poverty or appreciate anything on this
          website, please free free to donate to me at <a href="https://www.paypal.com/paypalme2/rylanschaeffer">
          https://www.paypal.com/paypalme2/rylanschaeffer</a>.
        </p>
      </footer>
    </div>

    
  </body>
</html>
