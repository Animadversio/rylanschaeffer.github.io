# Natural Language Processing

Disclaimer: Most of this was stolen from the HuggingFace documentation. These
distilled notes were a learning exercise for myself.

## Fundamentals
- [Types of Tasks](natural_language_processing/fundamentals/types_of_tasks.md)


## Datasets
- [GLUE](natural_language_processing/datasets/glue.md)
- [NewsQA](natural_language_processing/datasets/newsqa.md)
- [SQuAD](natural_language_processing/datasets/squad.md)
- [Wikitext](natural_language_processing/datasets/wikitext.md)

## Models

- [BERT](natural_language_processing/models/bert.md)
- [ELMo](natural_language_processing/models/elmo.md)
- 

## Papers
- [Xu et al. ACL 2020. Curriculum Learning for Natural Language Understanding](natural_language_processing/papers/xu_acl_2020_curriculum_learning/summary.html)
- [Campos. Arxiv 2021. Curriculum Learning for Language Modeling](natural_language_processing/papers/campos_arxiv_2021_curriculum_learning/summary.md)
- [Sanh et al. EMC @ NeurIPS 2019. DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter](natural_language_processing/papers/sanh_emc_2019_distilbert/summary.md)

## Preprocessing

- [TF-IDF](natural_language_processing/preprocessing/tf_idf.md)
