<!DOCTYPE html>
<html>

  <head>
    <meta charset='utf-8'>
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <meta name="viewport" content="width=device-width,maximum-scale=2">
    <meta name="description" content="Rylan Schaeffer">

    <link rel="stylesheet" type="text/css" media="all" href="../../stylesheets/stylesheet.css">

    <title></title>

    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-94033137-1', 'auto');
      ga('send', 'pageview');
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
    </script>

    <script type="text/javascript"
      src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({ TeX: { equationNumbers: {autoNumber: "AMS"} } });
    </script>

    <div style="display: none">
      $\DeclareMathOperator*{\argmax}{argmax}$
      $\DeclareMathOperator{\defeq}{\stackrel{def}{=}}$
      $\DeclareMathOperator{\Tr}{Tr}$
      $\DeclareMathOperator{\rank}{rank}$
    </div>
  </head>

  <body>

    <!-- HEADER -->
    <div id="header_wrap" class="outer">
      <header class="inner">
        <h1 id="header_title">Rylan Schaeffer > Learning > Neural Networks</h1>
        <nav>
          <div>
            <a href="../../index.html">Home</a> |
            <a href="../learning.html">Learning</a>
          </div>
        </nav>
      </header>
    </div>

    <!-- MAIN CONTENT -->
    <div id="main_content_wrap" class="outer">
      <section id="main_content" class="inner">

        <div id="toc_container">
          <h1 class="toc_title">Contents</h1>
          <ul class="table-of-content" id="markdown-toc">
            <li><a href="#activation_functions">Activation Functions</a>
              <ul>
                <li><a href="#logistic_sigmoid">Logistic Sigmoid </a></li>
                <li><a href="#hyperbolic_tangent">Hyperbolic Tangent</a></li>
                <li><a href="#relationship_sigmoid_tanh">Relationship between
                  Logistic Sigmoid and Hyperbolic Tangent</a></li>
              </ul>
            </li>
            <li><a href="#linear_nn">Linear Neural Networks</a>
              <ul>
                <li><a href="#deep_linear_nn">Deep Linear Neural Networks</a></li>
              </ul>
            </li>

          </ul>
        </div>


        <h2 id="activation_functions">Activation Functions</h2>
          <h3 id="logistic_sigmoid">Logistic Sigmoid</h3>
          <p>
            The logistic sigmoid function $\sigma(x) = \frac{1}{1+e^{-x}} = \frac{e^x}{e^x + 1}$.
          </p>

          <h3 id="hyperbolic_tangent">Hyperbolic Tangent</h3>
          <p>
            The hyperbolic tangent function $\tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}
            = \frac{e^{2x} - 1}{e^{2x} + 1}$.
          </p>
          <h3 id="relationship_sigmoid_tanh">Relationship between Logistic Sigmoid
            and Hyperbolic Tangent</h3>
            <p>
              The logistic sigmoid and the hyperbolic tangent functions can be
              expressed as transformation of each other. Specifically,
              $
              \begin{align}
              \tanh(x) &\defeq \frac{e^{2x} - 1}{e^{2x} + 1}
              = \frac{2e^{2x}}{e^{2x} + 1} - \frac{e^{2x} + 1}{e^{2x} + 1}
              = 2 \frac{e^{2x}}{e^{2x}} \frac{1}{1 + e^{-2x}} - 1
              = 2 \sigma(2x) - 1
              \end{align}
              $
            </p>

          <p>
            And therefore equivalently:
            $
            \begin{align}
            \sigma(x) = \frac{\tanh(x/2) + 1}{2}
            \end{align}
            $
          </p>

          <p>
            The consequence of this relationship is that the choice of activation
            function between the two doesn't much matter since each is capable of
            representing the one. That is, if a single scalar output from
            a single layer of a network using $\sigma()$ is
            $y(x, w) = w_0 + \sum w_i \sigma(x)$, then we can construct an equivalent
            network using $\tanh$ by defining $y(x, w') = w_0' + \sum w_i' \tanh(x/2)$
            where $w_0' = w_0 + \sum w_i / 2$ and $w_i' = w_i / 2$ for $i \neq 0$. Then
            $y(x, w) = y(x, w')$.
          </p>

        <h2 id="linear_nn">Linear Neural Networks</h2>
        <p>
          One of the simplest models of a neuron we might consider is a single "neuron" outputting
          a weighted combination of input features:

        </p>

        <p>
          $
          \begin{align}
          y = w^T x
          \end{align}
          $
        </p>

        <p>
          where $y \in \mathbb{R}$ and $w, x \in \mathbb{R}^d$.  This model, as a supervised learning
          problem under the Mean-Squared Error loss function, has been extensively studied. In such a setting, the
          neuron's goal is to learn a mapping $x \rightarrow y$ from a dataset of N x-y pairs,
          $\{(x_n, y_n) \}_{n=1}^N$, by minimizing the MSE $L(w) \defeq \frac{1}{N} \sum_{n=1}^N
          (y_n - w^T x_n)^2$. Does this model have a unique fixed point $w^*$ that minimizes the
          loss? We take the gradient and set equal to zero.
        </p>

        <p>
          $
          \begin{align}
          L(w) &\defeq \frac{1}{N} \sum_{n=1}^N (y_n - w^T x_n)^2 = \sum_{n=1}^N Tr(y_n - w^T x_n)^2\\\\
          \nabla_w L(w) = 0 &= \nabla_w \frac{1}{N} \sum_{n=1}^N (y_n^2 - 2 w^T x_n y_n + w^T x_n x_n^T w)\\
          0 &= -\sum_{n=1}^N y_n x_n + \sum_{n=1}^N x_n x_n^T w^*\\
          \sum_{n=1}^N x_n x_n^T w^* &= \sum_{n=1}^N y_n x_n
          \end{align}
          $
        </p>

        <p>
          The first observation in that $\sum_{n=1}^N x_n x_n^T$ is the empiric input-input
          covariance matrix and $\sum_{n=1}^N  y_n x_n$ is the empiric output-input covariance
          "matrix" (although here the covariance is a vector since $y$ is a scalar). The
          second observation is that $C \defeq \sum_{n=1}^N x_n x_n^T$ is the sum of $N$ rank-$D$
          matrices, meaning that $\rank(C) \leq min(N, D)$.

<!--          The second observation is that $w$ is a minimum because the Hessian of the loss function-->
<!--          $H_{ij} = \frac{\partial L(w)}{\partial w_i \partial w_j} = \sum_{n=1}^N x_n x_n^T$.-->
<!--          Because <a href="linear_algebra.html#covariance_pos_semidefinite">covariance matrices are always-->
<!--          positive-semidefinite</a>, the input-input covariance eigenvalues are $\geq 0$-->
        </p>

        <p>
          Two regimes emerge: $N < D$ and $N > D$. In the first regime, where the number of
          observations $N$ is less than the dimensionality of the data $D$, $C$ is a $D \times D$
          matrix of at most rank $N$, meaning that $C$ is not invertible. The easiest way to see this
          is to note that we have $N$ equations and $D$ unknowns, meaning we have fewer equations
          than unknowns. Consequently, in this regime, the training error is guaranteed to be 0.
          We can prove via construction that training error in this regime is zero
        </p>

        <h3 id="deep_linear_nn">Deep Linear Neural Networks</h3>
        <p>
          A deep linear neural network is a multi-layered neural network with the non-linear
          "activations" removed. That is, it is a composition of linear
          transformations (typically matrices). For instance,
          $\hat{y} = W_{32} W_{21} x$ is a two-layer linear network. Despite
          lacking the ability to learn non-linear functions, this class of
          network is attractive because it can be tractably analyzed and
          because its behaves similarly to its non-linear cousins under mild
          conditions.
        </p>

        <h3>For a two-layer linear neural network trained under mean squared
          error (MSE) on dataset $\{(x_i, y_i)\}_1^N$, what are the coupled differential
          equations that describe the network's learning dynamics?</h3>
        <p>
          Let $x \in \mathbb{R}^i$ be the input, $y \in \mathbb{R}^o$ be the
          output, and $L = \frac{1}{N} \sum_{i=1}^N (y - W_{32} W_{21} x)^T (y - W_{32} W_{21} x)$
          be the objective function. Define the input-input correlation matrix
          $\Sigma_{11} = \frac{1}{N}\sum_{i=1}^N x x^T$ and the input-output
          correlation matrix $\Sigma_{31} = \frac{1}{N}\sum_{i=1}^N y x^T$. We first
          derive $-\frac{\partial L}{\partial W_{32}}$:
        </p>

        <p>
          $
          \begin{align}
          -\frac{\partial L}{\partial W_{32}} &= -\frac{1}{\partial W_{32}} \frac{1}{N} \sum_{i=1}^N (y - W_{32} W_{21} x)^T (y - W_{32} W_{21} x) \nonumber \\
          &= -\frac{1}{\partial W_{32}} \frac{1}{N} \sum_{i=1}^N tr(y^T y - 2 y^T W_{32} W_{21} x + x^T W_{21}^T W_{32}^T W_{32} W_{21} x) \nonumber \\
          &= \frac{1}{N} \sum_{i=1}^N 2 \frac{1}{\partial W_{32}} tr(y^T W_{32} W_{21} x) - \frac{1}{\partial W_{32}}tr(x^T W_{21}^T W_{32}^T W_{32} W_{21} x) \nonumber \\
          &= \frac{1}{N} \sum_{i=1}^N 2 y x^T W_{21}^T - 2 W_{32} W_{21} x x^T W_{21}^T \nonumber \\
          &= 2 \Sigma_{31} W_{21}^T - 2 W_{32} W_{21} \Sigma_{11} W_{21}^T \nonumber \\
          -\frac{\partial L}{\partial W_{32}} &= 2 (\Sigma_{31} - W_{32} W_{21} \Sigma_{11}) W_{21}^T
          \end{align}
          $
        </p>

        <p>
          We similarly derive $-\frac{\partial L}{\partial W_{21}}$:
          $
          \begin{align}
          -\frac{\partial L}{\partial W_{21}} = 2 \Sigma_{32}^T (\Sigma_{31} - W_{32} W_{21} \Sigma_{11})
          \end{align}
          $
        </p>

        <p>
          Together, these coupled nonlinear differential equations define the
          learning dynamics of a linear neural network. One key aspect is that
          the learning dynamics are non-linear, meaning that deep linear networks
          are more sophisticated than shallow (i.e. single-layer) networks. These
          non-linear learning dynamics arise from the choice of error function:
          MSE produces a loss function that is quartic with respect to the weights,
          resulting in a gradient that is cubic with respect to the weights.
        </p>
      </section>
    </div>

    <!--FOOTER  -->
    <div id="footer_wrap" class="outer">
      <footer class="inner">
        <h3 id="footer_title">Rylan Schaeffer</h3>
        <p>
          I appreciate any and all feedback. If I've made an error or if you have a suggestion, please
          email me at <a href="mailto:rylanschaeffer@gmail.com">rylanschaeffer@gmail.com</a>.
        </p>
      </footer>
    </div>

    
  </body>
</html>
