<!DOCTYPE html>
<html>

  <head>
    <meta charset='utf-8'>
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <meta name="viewport" content="width=device-width,maximum-scale=2">
    <meta name="description" content="Rylan Schaeffer">

    <link rel="stylesheet" type="text/css" media="all" href="../../stylesheets/stylesheet.css">

    <title></title>

    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-94033137-1', 'auto');
      ga('send', 'pageview');
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
    </script>

    <script type="text/javascript"
      src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({ TeX: { equationNumbers: {autoNumber: "AMS"} } });
    </script>

    $\DeclareMathOperator*{\argmax}{argmax}$
    $\DeclareMathOperator{\defeq}{\stackrel{def}{=}}$
  </head>

  <body>

    <!-- HEADER -->
    <div id="header_wrap" class="outer">
      <header class="inner">
        <h1 id="header_title">Rylan Schaeffer > Learning > Neural Networks</h1>
        <nav>
          <div>
            <a href="../learning.html">Learning</a>
          </div>
        </nav>
      </header>
    </div>

    <!-- MAIN CONTENT -->
    <div id="main_content_wrap" class="outer">
      <section id="main_content" class="inner">

        <div id="toc_container">
          <h1 class="toc_title">Contents</h1>
          <ul class="table-of-content" id="markdown-toc">
            <li><a href="#activation_functions">Activation Functions</a></li>
              <ul>
                <li><a href="#logistic_sigmoid">Logistic Sigmoid </a></li>
                <li><a href="#hyperbolic_tangent">Hyperbolic Tangent</a></li>
                <li><a href="#relationship_sigmoid_tanh">Relationship between
                  Logistic Sigmoid and Hyperbolic Tangent</a></li>
              </ul>
            <li><a href="#deep_linear_nn">Deep Linear Neural Networks</a></li>
          </ul>
        </div>


        <h2 id="activation_functions">Activation Functions</h2>
          <h3 id="logistic_sigmoid">Logistic Sigmoid</h3>
          <p>
            The logistic sigmoid function $\sigma(x) = \frac{1}{1+e^{-x}} = \frac{e^x}{e^x + 1}$.
          </p>

          <h3 id="hyperbolic_tangent">Hyperbolic Tangent</h3>
          <p>
            The hyperbolic tangent function $\tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}
            = \frac{e^{2x} - 1}{e^{2x} + 1}$.
          </p>
          <h3 id="relationship_sigmoid_tanh">Relationship between Logistic Sigmoid
            and Hyperbolic Tangent</h3>
            <p>
              The logistic sigmoid and the hyperbolic tangent functions can be
              expressed as transformation of each other. Specifically,
              $
              \begin{align}
              \tanh(x) &\defeq \frac{e^{2x} - 1}{e^{2x} + 1}
              = \frac{2e^{2x}}{e^{2x} + 1} - \frac{e^{2x} + 1}{e^{2x} + 1}
              = 2 \frac{e^{2x}}{e^{2x}} \frac{1}{1 + e^{-2x}} - 1
              = 2 \sigma(2x) - 1
              \end{align}
              $
            </p>

          <p>
            And therefore equivalently:
            $
            \begin{align}
            \sigma(x) = \frac{\tanh(x/2) + 1}{2}
            \end{align}
            $
          </p>

          <p>
            The consequence of this relationship is that the choice of activation
            function between the two doesn't much matter since each is capable of
            representing the one. That is, if a single scalar output from
            a single layer of a network using $\sigmoid()$ is
            $y(x, w) = w_0 + \sum w_i \sigma(x)$, then we can construct an equivalent
            network using $\tanh$ by defining $y(x, w') = w_0' + \sum w_i' \tanh(x/2)$
            where $w_0' = w_0 + \sum w_i / 2$ and $w_i' = w_i / 2$ for $i \neq 0$. Then
            $y(x, w) = y(x, w')$.
          </p>

        <h2 id="deep_linear_nn">Deep Linear Neural Networks</h2>
        <h3>What is a deep linear neural network? Why does it matter?</h3>
        <p>
          A deep linear neural network is a neural network with the non-linear
          "activations" removed. That is, it is a composition of linear
          transformations (typically matrices). For instance,
          $\hat{y} = W_{32} W_{21} x$ is a two-layer linear network. Despite
          lacking the ability to learn non-linear functions, this class of
          network is attractive because it can be tractably analyzed and
          because its behaves similarly to its non-linear cousins under mild
          conditions.
        </p>

        <h3>For a two-layer linear neural network trained under mean squared
          error (MSE) on dataset $\{(x_i, y_i)\}_1^N$, what are the coupled differential
          equations that describe the network's learning dynamics?</h3>
        <p>
          Let $x \in \mathbb{R}^i$ be the input, $y \in \mathbb{R}^o$ be the
          output, and $L = \frac{1}{N} \sum_{i=1}^N (y - W_{32} W_{21} x)^T (y - W_{32} W_{21} x)$
          be the objective function. Define the input-input correlation matrix
          $\Sigma_{11} = \frac{1}{N}\sum_{i=1}^N x x^T$ and the input-output
          correlation matrix $\Sigma_{31} = \frac{1}{N}\sum_{i=1}^N y x^T$. We first
          derive $-\frac{\partial L}{\partial W_{32}}$:
        </p>

        <p>
          $
          \begin{align}
          -\frac{\partial L}{\partial W_{32}} &= -\frac{1}{\partial W_{32}} \frac{1}{N} \sum_{i=1}^N (y - W_{32} W_{21} x)^T (y - W_{32} W_{21} x) \nonumber \\
          &= -\frac{1}{\partial W_{32}} \frac{1}{N} \sum_{i=1}^N tr(y^T y - 2 y^T W_{32} W_{21} x + x^T W_{21}^T W_{32}^T W_{32} W_{21} x) \nonumber \\
          &= \frac{1}{N} \sum_{i=1}^N 2 \frac{1}{\partial W_{32}} tr(y^T W_{32} W_{21} x) - \frac{1}{\partial W_{32}}tr(x^T W_{21}^T W_{32}^T W_{32} W_{21} x) \nonumber \\
          &= \frac{1}{N} \sum_{i=1}^N 2 y x^T W_{21}^T - 2 W_{32} W_{21} x x^T W_{21}^T \nonumber \\
          &= 2 \Sigma_{31} W_{21}^T - 2 W_{32} W_{21} \Sigma_{11} W_{21}^T \nonumber \\
          -\frac{\partial L}{\partial W_{32}} &= 2 (\Sigma_{31} - W_{32} W_{21} \Sigma_{11}) W_{21}^T
          \end{align}
          $
        </p>

        <p>
          We similarly derive $-\frac{\partial L}{\partial W_{21}}$:
          $
          \begin{align}
          -\frac{\partial L}{\partial W_{21}} = 2 \Sigma_{32}^T (\Sigma_{31} - W_{32} W_{21} \Sigma_{11})
          \end{align}
          $
        </p>

        <p>
          Together, these coupled nonlinear differential equations define the
          learning dynamics of a linear neural network. One key aspect is that
          the learning dynamics are non-linear, meaning that deep linear networks
          are more sophisticated than shallow (i.e. single-layer) networks. These
          non-linear learning dynamics arise from the choice of error function:
          MSE produces a loss function that is quartic with respect to the weights,
          resulting in a gradient that is cubic with respect to the weights.
        </p>
      </section>
    </div>

    <!--FOOTER  -->
    <div id="footer_wrap" class="outer">
      <footer class="inner">
        <h3 id="footer_title">Rylan Schaeffer</h3>
        <p>
          I appreciate any and all feedback. If I've made an error or if you have a suggestion, please
          email me at <a href="mailto:rylanschaeffer@gmail.com">rylanschaeffer@gmail.com</a>.
        </p>
      </footer>
    </div>

    
  </body>
</html>
