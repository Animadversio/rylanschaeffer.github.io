<!DOCTYPE html>
<html lang="en-US">

  <head>
    <meta charset='utf-8'>
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <meta name="viewport" content="width=device-width,maximum-scale=2">
    <meta name="description" content="Rylan Schaeffer">

    <link rel="stylesheet" type="text/css" media="all" href="../../stylesheets/stylesheet.css">

    <title></title>

    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-94033137-1', 'auto');
      ga('send', 'pageview');
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
    </script>

    <script type="text/javascript"
      src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({ TeX: { equationNumbers: {autoNumber: "AMS"} } });
    </script>

    <div style="display: none">
      $\DeclareMathOperator*{\argmax}{argmax}$
      $\DeclareMathOperator{\defeq}{\stackrel{def}{=}}$
      $\DeclareMathOperator{\Tr}{Tr}$
      $\DeclareMathOperator{\rank}{rank}$
      $\DeclareMathOperator{\sign}{sign}$
    </div>
  </head>

  <body>

    <!-- HEADER -->
    <div id="header_wrap" class="outer">
      <header class="inner">
        <h1 id="header_title">Rylan Schaeffer > Learning > Neural Networks</h1>
        <nav>
          <div>
            <a href="../../index.html">Home</a> |
            <a href="../learning.html">Learning</a>
          </div>
        </nav>
      </header>
    </div>

    <!-- MAIN CONTENT -->
    <div id="main_content_wrap" class="outer">
      <section id="main_content" class="inner">

        <div id="toc_container">
          <h1 class="toc_title">Contents</h1>
          <ul class="table-of-content" id="markdown-toc">
            <li><a href="#learning_algorithms">Learning Algorithms</a>
              <ul>
                <li><a href="#backprop">Backpropagation</a></li>
                <li><a href="#weight_perturbation">Weight Perturbation</a></li>
                <li><a href="#node_pertubation">Node Perturbation</a></li>
                <li><a href="#bp_vs_wp_vs_np">Comparison of Backprop, Weight Perturbation & Node Perturbation</a></li>
              </ul>
            </li>

            <li><a href="#linear_models">Linear Models</a>
              <ul>
                <li><a href="#linear_neuron">Single Linear Neuron</a></li>
                <li><a href="#deep_linear_nn">Deep Linear Neural Networks</a></li>
              </ul>
            </li>

            <li><a href="#nonlinear_models">Nonlinear Neural Networks</a>
                <ul>
                    <li><a href="#perceptron">Perceptron</a></li>
                </ul>
            </li>

            <li><a href="#activation_functions">Activation Functions</a>
              <ul>
                <li><a href="#logistic_sigmoid">Logistic Sigmoid </a></li>
                <li><a href="#hyperbolic_tangent">Hyperbolic Tangent</a></li>
                <li><a href="#relationship_sigmoid_tanh">Relationship between
                  Logistic Sigmoid and Hyperbolic Tangent</a></li>
              </ul>
            </li>
          </ul>
        </div>

        <h2 id="learning_algorithms">Learning Algorithms</h2>
        <div>
          <h3 id="backprop">Backpropagation</h3>
          <div>
            <p>
              Backpropagation ("backprop" for short)
              TODO
            </p>

            <h4>Backprop as Chain Derivative</h4>
            <p>
              TODO
            </p>

            <h4>Backprop as Constrained Optimization</h4>
            <p>
              Backprop is most commonly derived using chain derivatives.
              However, backprop can also be derived as the solution to a constrained optimization
              problem (<a href="papers/leCun_1988_Theoretical_Framework_For_Backprop.pdf">LeCun 1988</a>).
              The idea is to see backprop as an algorithm for selecting a set of vectors
              $\{x_i\}_{i=1}^{L+1}$, one per layer of the network, that minimize a loss function
              subject to a set of consistency equations:

              $$ x^l = f(W^l x^{l-1} + b^l)$$

              or equivalently, written in index form:

              $$ x_i^l = f \Big( \sum_{j=1}^{N^{l-1}} W_{ij}^l x_j^{l-1} + b_i^l \Big)$$
            </p>

            <p>
              we have $N$ datapoints $\{x_i, y_i\}_{i=1}^N$ and a
              mean-squared error loss function.
            </p>
          </div>

          <h3 id="weight_perturbation">Weight Perturbation</h3>
          <div>
            <p>
              Weight Perturbation is an algorithm for learning with a global error signal
              whereby additive noise is used to estimate the gradient of
              the error signal with respect to that parameter
              (<a href="papers/Jabri_Flower_1992_Weight_Perturbation.pdf">Jabri & Flower 1992</a>).
              Consider training a feedforward network $y(\cdot)$ using mean squared error on training data
              $\{(x_i, y_i)\}_{i=1}^N$:

              $$ E_{tr} = \frac{1}{2} || y_i - y(x_i) ||_2^2 $$

              $$x_i^l = f \Big(\sum_j W_{ij}^l x_j^{l-1} + b_i^l \Big) $$

              Through a relatively simple algorithm, we can estimate $\frac{\partial E_{tr}}{\partial W_{ij}^l}$
              and $\frac{\partial E_{tr}}{\partial b_{i}^l}$ simply by adding a small amount of
              independent noise to each parameter. To understand why this works, consider adding
              noise $\Omega_{ij}^l, \gamma_i^l \sim N(0, \sigma^2)$ to each parameter $W_{ij}^l, b_i^l$.
              The network's activity and outputs will consequently be slightly pertubed:

              $$ \tilde{E}_{tr} = \frac{1}{2} || y_i - \tilde{y}(x_i) ||_2^2$$

              $$\tilde{x}_i^l = f \Big(\sum_j (W_{ij}^l + \Omega_{ij}^l) \tilde{x}_j^{l-1} + (b_i^l + \gamma_i^l) \Big) $$
            </p>

            <p>
              If the variance of the additive noise $\sigma^2$ is small, we can use a linear
              approximation of the training error $E_{tr}$ to quantify how the additive noise will affect the
              training error.

              $$\tilde{E}_{tr} \approx E_{tr} + \sum_{l, i, j} \frac{\partial E}{\partial W_{ij}^l}
              \Omega_{ij}^l + \sum_{l, ij} \frac{\partial E}{\partial b_i^l} \gamma_i^l$$

              Rearranging slightly, we can write that the difference between the pertubed and
              unperturbed training error is:

              $$\tilde{E}_{tr} - E_{tr} \approx \sum_{l, i, j} \frac{\partial E}{\partial W_{ij}^l}
              \Omega_{ij}^l + \sum_{l, ij} \frac{\partial E}{\partial b_i^l} \gamma_i^l$$

              Since each noise term was sampled independently from a distribution with mean 0,
              we see that if we multiply both sides by a noise term $\Omega_{ab}^c$ and take the expectation with
              respect to that noise term, the independence and zero mean will annihilate all
              terms except the one containing the gradient with respect to the corresponding
              parameter $W_{ab}^c$:

              $\begin{align}
              \tilde{E}_{tr} - E_{tr}
              &\approx \sum_{l, i, j} \frac{\partial E}{\partial W_{ij}^l}
              \Omega_{ij}^l + \sum_{l, ij} \frac{\partial E}{\partial b_i^l} \gamma_i^l\\
              (\tilde{E}_{tr} - E_{tr}) \Omega_{ab}^c
              &\approx \sum_{l, i, j} \frac{\partial E}{\partial W_{ij}^l}
              \Omega_{ij}^l \Omega_{ab}^c  + \sum_{l, ij} \frac{\partial E}{\partial b_i^l} \gamma_i^l
              \Omega_{ab}^c\\
              \langle (\tilde{E}_{tr} - E_{tr}) \Omega_{ab}^c \rangle_{\Omega_{ab}^c}
              &\approx \langle \sum_{l, i, j} \frac{\partial E}{\partial W_{ij}^l}
              \Omega_{ij}^l \Omega_{ab}^c  + \sum_{l, ij} \frac{\partial E}{\partial b_i^l} \gamma_i^l
              \Omega_{ab}^c \rangle_{\Omega_{ab}^c}\\
              &\approx \sum_{l, i, j} \frac{\partial E}{\partial W_{ij}^l}
              \langle \Omega_{ij}^l \Omega_{ab}^c \rangle_{\Omega_{ab}^c}  + \sum_{l, ij}
              \frac{\partial E}{\partial b_i^l} \gamma_i^l \langle \Omega_{ab}^c \rangle_{\Omega_{ab}^c}\\
              &\approx \frac{\partial E}{\partial W_{ab}^c} \sigma^2\\
              \frac{1}{\sigma^2} \langle (\tilde{E}_{tr} - E_{tr}) \Omega_{ab}^c \rangle_{\Omega_{ab}^c}
              &\approx  \frac{\partial E}{\partial W_{ab}^c}
              \end{align}$
            </p>

            <p>
              We can further simplify by noting that since $E_{tr}$ is independent of $\Omega_{ab}^c$,

              $$ \langle E_{tr} \Omega_{ab}^c \rangle_{\Omega_{ab}^c} =
              E_{tr} \langle \Omega_{ab}^c \rangle_{\Omega_{ab}^c} = 0$$

              and therefore

              $$\frac{1}{\sigma^2} \langle (\tilde{E}_{tr} - E_{tr}) \Omega_{ab}^c \rangle_{\Omega_{ab}^c}
               = \frac{1}{\sigma^2} \langle \tilde{E}_{tr} \Omega_{ab}^c \rangle_{\Omega_{ab}^c}
              \approx \frac{\partial E}{\partial W_{ab}^c}$$

              This same argument applies for $\gamma_a^c$ as well, yielding:

              $$ \frac{1}{\sigma^2} \langle \tilde{E}_{tr} \gamma_{a}^c \rangle_{\gamma{a}^c}
              \approx \frac{\partial E}{\partial b_{a}^c}$$
            </p>

            <p>
              What we've shown is that the covariance between the pertubation of the weight
              and the pertubed error will (in expectation) point in the direction of the
              gradient. No chain derivatives necessary!
            </p>
          </div>

          <h3 id="node_perturbation">Node Perturbation</h3>
          <div>
            <p>
              TODO
            </p>
          </div>
        </div>

        <h2 id="activation_functions">Activation Functions</h2>
        <div>
          <h3 id="logistic_sigmoid">Logistic Sigmoid</h3>
          <div>

          <p>
            The logistic sigmoid function $\sigma(x) = \frac{1}{1+e^{-x}} = \frac{e^x}{e^x + 1}$.
          </p>

          </div>

          <h3 id="hyperbolic_tangent">Hyperbolic Tangent</h3>
          <div>
          <p>
            The hyperbolic tangent function $\tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}
            = \frac{e^{2x} - 1}{e^{2x} + 1}$.
          </p>
          </div>

          <h3 id="relationship_sigmoid_tanh">Relationship between Logistic Sigmoid
            and Hyperbolic Tangent</h3>
          <div>
            <p>
              The logistic sigmoid and the hyperbolic tangent functions can be
              expressed as transformation of each other. Specifically,
              $
              \begin{align}
              \tanh(x) &\defeq \frac{e^{2x} - 1}{e^{2x} + 1}
              = \frac{2e^{2x}}{e^{2x} + 1} - \frac{e^{2x} + 1}{e^{2x} + 1}
              = 2 \frac{e^{2x}}{e^{2x}} \frac{1}{1 + e^{-2x}} - 1
              = 2 \sigma(2x) - 1
              \end{align}
              $
            </p>

          <p>
            And therefore equivalently:
            $
            \begin{align}
            \sigma(x) = \frac{\tanh(x/2) + 1}{2}
            \end{align}
            $
          </p>

          <p>
            The consequence of this relationship is that the choice of activation
            function between the two doesn't much matter since each is capable of
            representing the one. That is, if a single scalar output from
            a single layer of a network using $\sigma()$ is
            $y(x, w) = w_0 + \sum w_i \sigma(x)$, then we can construct an equivalent
            network using $\tanh$ by defining $y(x, w') = w_0' + \sum w_i' \tanh(x/2)$
            where $w_0' = w_0 + \sum w_i / 2$ and $w_i' = w_i / 2$ for $i \neq 0$. Then
            $y(x, w) = y(x, w')$.
          </p>
          </div>
          </div>

        <h2 id="linear_models">Linear Models</h2>
        <div>
          <h3 id="linear_neuron">Single Linear Neuron</h3>
          <div>

            <p>
              One of the simplest models of a neuron we might consider is a single "neuron" outputting
              a weighted combination of input features:
            </p>

            <p>
              $
              \begin{align}
              y = w^T x
              \end{align}
              $
            </p>

            <p>
              where $y \in \mathbb{R}$ and $w, x \in \mathbb{R}^d$.  This model, as a supervised learning
              problem under the Mean-Squared Error loss function, has been extensively studied. In such a setting, the
              neuron's goal is to learn a mapping $x \rightarrow y$ from a dataset of N x-y pairs,
              $\{(x_n, y_n) \}_{n=1}^N$, by minimizing the training MSE $L^{tr}(w) \defeq \frac{1}{N} \sum_{n=1}^N
              (y_n - w^T x_n)^2$. Does this model have a unique fixed point $w^*$ that minimizes the
              loss? We take the gradient and set equal to zero.
            </p>

            <p>
              $
              \begin{align}
              L^{tr}(w) &\defeq \frac{1}{N} \sum_{n=1}^N (y_n - w^T x_n)^2 = \sum_{n=1}^N Tr(y_n - w^T x_n)^2\\\\
              \nabla_w L(w) = 0 &= \nabla_w \frac{1}{N} \sum_{n=1}^N (y_n^2 - 2 w^T x_n y_n + w^T x_n x_n^T w)\\
              0 &= -\sum_{n=1}^N y_n x_n + \sum_{n=1}^N x_n x_n^T w^*\\
              \sum_{n=1}^N x_n x_n^T w^* &= \sum_{n=1}^N y_n x_n
              \end{align}
              $
            </p>

            <p>
                One important observation in that $\sum_{n=1}^N x_n x_n^T$ is the empiric input-input
                covariance matrix and $\sum_{n=1}^N  y_n x_n$ is the empiric output-input covariance
                "matrix" (although here the covariance is a vector since $y$ is a scalar), and the
                optimal weight vector $w^*$ relates these two covariance matrices. Another important
                observation is that $C \defeq \sum_{n=1}^N x_n x_n^T$ is the sum of $N$ rank-$D$
                matrices, meaning that $\rank(C) \leq min(N, D)$. The third important observation is that the
                Hessian $H_{ij} \defeq \frac{\partial L^{tr}(w)}{\partial w_i \partial w_j}= C_{ij}$.
                More on this in a bit.
            </p>

            <p>
                The first question to ask is what behavior we should expect from such a model?
                Two regimes emerge, one where $N < D$ and the other where $N \geq D$. In the first regime,
                where the number of observations $N$ is less than the dimensionality of the data $D$, $C$
                is a $D \times D$ matrix of at most rank $N$, meaning that $C$ is not invertible and
                therefore infinitely many solution exist. The easiest way to see this
                is to note that we have $N$ equations and $D$ unknowns, meaning we have fewer equations
                than unknowns. Consequently, in this regime, the training error $L^{tr}(w)$ is guaranteed to be 0.
                For those unconvinced, we can prove this claim via construction:
            </p>

            <p>
                <!--          The second observation is that $w$ is a minimum because the Hessian of the loss function-->
                <!--          $H_{ij} = \frac{\partial L(w)}{\partial w_i \partial w_j} = \sum_{n=1}^N x_n x_n^T$.-->
                <!--          Because <a href="linear_algebra.html#covariance_pos_semidefinite">covariance matrices are always-->
                <!--          positive-semidefinite</a>, the input-input covariance eigenvalues are $\geq 0$-->
            </p>
            </div>

          <h3 id="deep_linear_nn">Deep Linear Neural Networks</h3>
          <div>
            <p>
              A deep linear neural network is a multi-layered neural network with the non-linear
              "activations" removed. That is, it is a composition of linear
              transformations (typically matrices). For instance,
              $\hat{y} = W_{32} W_{21} x$ is a two-layer linear network. Despite
              lacking the ability to learn non-linear functions, this class of
              network is attractive because it can be tractably analyzed and
              because its behaves similarly to its non-linear cousins under mild
              conditions.
            </p>

            <p>
              One question we might immediately ask is <b>for a two-layer linear neural network trained
              under mean squared
                error (MSE) on dataset $\{(x_i, y_i)\}_1^N$, what are the coupled differential
                equations that describe the network's learning dynamics?</b>
              Let $x \in \mathbb{R}^i$ be the input, $y \in \mathbb{R}^o$ be the
              output, and $L = \frac{1}{N} \sum_{i=1}^N (y - W_{32} W_{21} x)^T (y - W_{32} W_{21} x)$
              be the objective function. Define the input-input correlation matrix
              $\Sigma_{11} = \frac{1}{N}\sum_{i=1}^N x x^T$ and the input-output
              correlation matrix $\Sigma_{31} = \frac{1}{N}\sum_{i=1}^N y x^T$. We first
              derive $-\frac{\partial L}{\partial W_{32}}$:
            </p>

            <p>
              $
              \begin{align}
              -\frac{\partial L}{\partial W_{32}} &= -\frac{1}{\partial W_{32}} \frac{1}{N} \sum_{i=1}^N (y - W_{32} W_{21} x)^T (y - W_{32} W_{21} x) \nonumber \\
              &= -\frac{1}{\partial W_{32}} \frac{1}{N} \sum_{i=1}^N tr(y^T y - 2 y^T W_{32} W_{21} x + x^T W_{21}^T W_{32}^T W_{32} W_{21} x) \nonumber \\
              &= \frac{1}{N} \sum_{i=1}^N 2 \frac{1}{\partial W_{32}} tr(y^T W_{32} W_{21} x) - \frac{1}{\partial W_{32}}tr(x^T W_{21}^T W_{32}^T W_{32} W_{21} x) \nonumber \\
              &= \frac{1}{N} \sum_{i=1}^N 2 y x^T W_{21}^T - 2 W_{32} W_{21} x x^T W_{21}^T \nonumber \\
              &= 2 \Sigma_{31} W_{21}^T - 2 W_{32} W_{21} \Sigma_{11} W_{21}^T \nonumber \\
              -\frac{\partial L}{\partial W_{32}} &= 2 (\Sigma_{31} - W_{32} W_{21} \Sigma_{11}) W_{21}^T
              \end{align}
              $
            </p>

            <p>
              We similarly derive $-\frac{\partial L}{\partial W_{21}}$:
              $
              \begin{align}
              -\frac{\partial L}{\partial W_{21}} = 2 \Sigma_{32}^T (\Sigma_{31} - W_{32} W_{21} \Sigma_{11})
              \end{align}
              $
            </p>

            <p>
              Together, these coupled nonlinear differential equations define the
              learning dynamics of a linear neural network. One key aspect is that
              the learning dynamics are non-linear, meaning that deep linear networks
              are more sophisticated than shallow (i.e. single-layer) networks. These
              non-linear learning dynamics arise from the choice of error function:
              MSE produces a loss function that is quartic with respect to the weights,
              resulting in a gradient that is cubic with respect to the weights.
            </p>
            </div>
          </div>

        <h2 id="nonlinear_models">Nonlinear Models</h2>
        <div>
              <h3 id="perceptron">Perceptron</h3>
              <div>
                  <p>
                      One of the earliest and simplest models of a nonlinear neural network is
                      a binary classifier called the Perceptron, the non-linear equivalent of the
                      <a href="#linear_neuron"> single linear neuron</a>:
                  </p>

                  <p>
                      $\begin{align}
                      y = \sign(w^T x)
                      \end{align}$
                  </p>

                  <p>
                      An amazing property of the perceptron was that for a given classification
                      problem, if the
                  </p>

              </div>
            </div>



      </section>
    </div>

    <!--FOOTER  -->
    <div id="footer_wrap" class="outer">
      <footer class="inner">
        <h3 id="footer_title">Rylan Schaeffer</h3>
        <p>
          I appreciate any and all feedback. If I've made an error or if you have a suggestion, please
          email me at <a href="mailto:rylanschaeffer@gmail.com">rylanschaeffer@gmail.com</a>.
        </p>
      </footer>
    </div>

    
  </body>
</html>
