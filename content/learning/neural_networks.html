<!DOCTYPE html>
<html>

  <head>
    <meta charset='utf-8'>
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <meta name="viewport" content="width=device-width,maximum-scale=2">
    <meta name="description" content="Rylan Schaeffer">

    <link rel="stylesheet" type="text/css" media="all" href="../../stylesheets/stylesheet.css">

    <title></title>

    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-94033137-1', 'auto');
      ga('send', 'pageview');
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
    </script>

    <script type="text/javascript"
      src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({ TeX: { equationNumbers: {autoNumber: "AMS"} } });
    </script>

    $\DeclareMathOperator*{\argmax}{argmax}$
  </head>

  <body>

    <!-- HEADER -->
    <div id="header_wrap" class="outer">
      <header class="inner">
        <h1 id="header_title">Learning: Neural Networks</h1>
        <nav>
          <div>
            <a href="../learning.html">Learning</a>
          </div>
        </nav>
      </header>
    </div>

    <!-- MAIN CONTENT -->
    <div id="main_content_wrap" class="outer">
      <section id="main_content" class="inner">
        <h2>(Deep) Linear Neural Networks</h2>
        <h3>What is a (deep) linear neural network? Why does it matter?</h3>
        <p>
          A (deep) linear neural network is a neural network with the non-linear
          "activations" removed. That is, it is a composition of linear
          transformations (typically matrices). For instance,
          $\hat{y} = W_{32} W_{21} x$ is a two-layer linear network. Despite
          lacking the ability to learn non-linear functions, this class of
          network is attractive because it can be tractably analyzed and
          because its behaves similarly to its non-linear cousins under mild
          conditions.
        </p>

        <h3>For a two-layer linear neural network trained with mean squared
          error for dataset $\{(x_i, y_i)\}_1^N$, what are the coupled differential
          equations that describe the network's learning dynamics?</h3>
        <p>
          Let $x \in \mathbb{R}^i$ be the input, $y \in \mathbb{R}^o$ be the
          output, and $L = \frac{1}{N} \sum_{i=1}^N (y - W_{32} W_{21} x)^T (y - W_{32} W_{21} x)$
          be the objective function.
        </p>

        <p>
          $
          \begin{align}
          -\frac{\partial L}{\partial W_{32}} &= -\frac{1}{\partial W_{32}} \frac{1}{N} \sum_{i=1}^N (y - W_{32} W_{21} x)^T (y - W_{32} W_{21} x)\\
          &= -\frac{1}{\partial W_{32}} \frac{1}{N} \sum_{i=1}^N y^T y - 2 y^T W_{32} W_{21} x + x^T W_{21}^T W_{32}^T W_{32} W_{21} x\\
          &= 
          \end{align}
          $
        </p>

      </section>
    </div>

    <!--FOOTER  -->
    <div id="footer_wrap" class="outer">
      <footer class="inner">
        <h3 id="footer_title">Rylan Schaeffer</h3>
        <p>
          I appreciate any and all feedback. If I've made an error or if you have a suggestion, please
          email me at <a href="mailto:rylanschaeffer@gmail.com">rylanschaeffer@gmail.com</a>.
        </p>
      </footer>
    </div>

    
  </body>
</html>
