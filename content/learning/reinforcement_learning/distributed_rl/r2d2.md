# Recurrent Replay Distributed DQN (R2D2)

The Recurrent Replay Distributed DQN (or R2D2 for short) paper asks two questions:

1. What happens if we replace [DQN](../value_based/dqn.md)'s feedforward architecture
    with a recurrent architecture?
2. How can we train such a model in a distributed manner?

## Background

[Ape-X](ape_x.md) used distributed replay to decouple learning from acting. Actors
would generate experiences for a shared replay buffer, while the learner receives
random training batches.

[IMPALA](impala.md) used an off-policy correction to learn from experiences in a 
First-In-First-Out (FIFO) queue.

## R2D2

R2D2 is similar to Ape-X in that it uses

- prioritized distributed replay
- n-step double Q-Learning
- Batches of replayed experiences

The biggest difference is that R2D2 adds an LSTM layer after DQN's convolutional stack.
Other smaller differences, which aren't explained or explored in-depth, include:

- Rewards are no longer clipped, but instead rescaled to 

$$\text{rescale}(r) = \text{sign}(r) * (\sqrt{|r| + 1} - 1) $$

- Replay prioritization for a given trajectory is defined as a mixture of max and
  mean absolute n-step TD errors:

$$\text{priority} = \eta \text{max}_t \delta_t + (1 - \eta) (\frac{1}{T} \sum_{t=1}^T \delta_t ) $$

TODO: check that max is indeed over steps within a trajectory


