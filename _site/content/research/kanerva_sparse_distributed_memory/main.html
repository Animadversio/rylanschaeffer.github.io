<!DOCTYPE html>
<html>

  <head>
    <meta charset='utf-8'>
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <meta name="viewport" content="width=device-width,maximum-scale=2">
    <meta name="description" content="Rylan Schaeffer">

    <link rel="stylesheet" type="text/css" media="all" href="../../../stylesheets/stylesheet.css">

    <title>Pentti Kanerva's Sparse Distributed Memory</title>

    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-94033137-1', 'auto');
      ga('send', 'pageview');
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
    </script>

    <script type="text/javascript"
      src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({ TeX: { equationNumbers: {autoNumber: "AMS"} } });
    </script>

    $\DeclareMathOperator*{\argmax}{argmax}$
  </head>

  <body>

    <!-- HEADER -->
    <div id="header_wrap" class="outer">
      <header class="inner">
        <h1 id="header_title">Explanation of:<br>Pentti Kanerva's Sparse Distributed Memory</h1>
        <h3 id="project_tagline"><a href="https://dl.acm.org/citation.cfm?id=534853">Pentti Kanerva 1988.</a></h3>
          <section id="quick_links">
            <a class="back" href="../../research.html"></a>
          </section>
      </header>
    </div>

    <!-- MAIN CONTENT -->
    <div id="main_content_wrap" class="outer">
      <section id="main_content" class="inner">
        <p>
          I've found that the overwhelming majority of online information in artificial intelligence
          research falls into one of two categories: the first is aimed at explaining advances to
            lay audiences, and the second is aimed at explaining advances to other researchers. I
            haven't found a good resource for people with a technical background who are unfamiliar
            with the more advanced concepts and are looking for someone to fill them in. This is my
            attempt to bridge that gap, by providing approachable yet (relatively) detailed explanations.
            In this post, I explain the titular book - <a href="https://dl.acm.org/citation.cfm?id=534853">Sparse Distributed Memory</a>.
        </p><br>

        <h2>Motivation</h2>
        <p>
          I discovered the concept of Pentti Kanerva's Sparse Distributed Memory (SDM) through two relatively
          recent papers (<a href="https://arxiv.org/abs/1804.01756">1</a>, <a href="https://papers.nips.cc/paper/8149-learning-attractor-dynamics-for-generative-memory">2</a>,
          but I personally felt that neither paper provided a crisp explanation of the motivation, construction,
          properties and tradeoffs of this SDM model, and the <a href="https://en.wikipedia.org/wiki/Sparse_distributed_memory">
          Wikipedia page</a> was packed with information, but failed to tell a story. Why had the authors
          chosen this so-called SDM as the basis for their work, eschewing their earlier efforts in the direction
          of the <a href="/home/rylan/Documents/rylanschaeffer.github.io/content/research/neural_turing_machine/main.html">Neural Turing Machine</a>
          and its successor, the Differentiable Neural Computer?
        </p>

        <h2>Background</h2>
        <p>
          The book begins with a lovely, lucid forward by <a href="https://en.wikipedia.org/wiki/Douglas_Hofstadter">Douglas Hofstadter</a>,
          who acknowledges that even Kanerva devotes insufficient time to explaining the motivations
          behind his SDM. In Kanerva's own words, he sought to "construct a physiologically plausible theory" for
          animal memory, but what were the actual observations he wanted his theory to be capable of explaining?
          Hofstadter composes a list of memory-related phenomena that provides a good starting point:
        <ul>
        <li>Rapid retrieval of specific instances</li>
        <li>Representation invariance. "Thus, when we see a staircase (say), no matter how big or small it
        is, no matter how twisted or straight, no matter how ornamented or plain, modern or old, dirty or
        clean, the label "staircase" spontaneously jumps to center stage without any conscious effort at all"</li>
        <li>Easy manipulation of concepts</li>
        <li>Similarities between </li>
        <li>Rapid connection of diverse elements</li>
        <li>Easy construction of analogies</li>
        <li>Phrase blends</li>
        <li>Tip of the tongue</li>
        <li>Metamemory. Know whether we know, know whether we don't know.</li>
        </ul>
        </p>


        <h2>Intuition</h2>

        <h2>Mathematics</h2>
        
        <h2>Experiments and Results</h2>

        <h2>Discussion</h2>

        <h2>Summary</h2>

        <h2>Notes</h2>
        <p>
          I appreciate any and all feedback. If I've made an error or if you have a suggestion, you
            can <a href="mailto:rylanschaeffer@gmail.com">email me</a> or comment on the
            <a href="">Reddit</a> or <a href="">HackerNews</a> threads.
        </p>
      </section>
    </div>

    <!-- FOOTER  -->
    <div id="footer_wrap" class="outer">
      <footer class="inner">
        
        <p class="copyright">Slate theme maintained by <a href="https://github.com/pages-themes">pages-themes</a></p>
        
        <p>Published with <a href="https://pages.github.com">GitHub Pages</a></p>
      </footer>
    </div>

    
  </body>
</html>
