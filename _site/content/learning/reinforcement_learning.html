<!DOCTYPE html>
<html>

  <head>
    <meta charset='utf-8'>
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <meta name="viewport" content="width=device-width,maximum-scale=2">
    <meta name="description" content="Rylan Schaeffer">

    <link rel="stylesheet" type="text/css" media="all" href="../../stylesheets/stylesheet.css">

    <title></title>

    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-94033137-1', 'auto');
      ga('send', 'pageview');
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
    </script>

    <script type="text/javascript"
      src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({ TeX: { equationNumbers: {autoNumber: "AMS"} } });
    </script>

    <div style="display: none">
      $\DeclareMathOperator*{\argmax}{argmax}$
      $\DeclareMathOperator{\defeq}{\stackrel{def}{=}}$
    </div>
  </head>

  <body>

    <!-- HEADER -->
    <div id="header_wrap" class="outer">
      <header class="inner">
        <h1 id="header_title">Rylan Schaeffer > Learning > Reinforcement Learning</h1>
        <nav>
          <div>
            <a href="../../index.html">Home</a> |
            <a href="../learning.html">Learning</a>
          </div>
        </nav>
      </header>
    </div>

    <!-- MAIN CONTENT -->
    <div id="main_content_wrap" class="outer">
      <section id="main_content" class="inner">

        <div id="toc_container">
        <h1 class="toc_title">Contents</h1>
        <ul class="table-of-content" id="markdown-toc">
          <li>Value-Based RL
            <ul>
              <li><a href="#rescorla_wagner_rule">Rescorla Wagner Learning Rule</a> </li>
            </ul>
          </li>
          <li><a href="#policy_based_rl">Policy-Based RL</a>
            <ul>
              <li><a href="#policy_gradient_derivation">Derivation of Policy Gradient</a></li>
            </ul>
          </li>
          <li><a href="#actor_critic_rl">Actor-Critic RL</a></li>
        </ul>
        </div>

        <h2 id="value_based_rl">Value-Based RL</h2>
        <div>
          <h3 id="rescorla_wagner_rule">Rescorla Wagner Learning Rule</h3>
          <div>
            <p>
              The Rescorla-Wagner Learning Rule (1972) was a seminal model of associative learning
              that preceded reinforcement learning. Associative learning is the problem of learning
              how different stimuli are associated with rewards or punishments $r_n$. The model considers
              the agent receiving a one-hot encoded stimulus vector $s_n$, where each element indicates
              the presence or absence of a stimulus and $n$ is the trial number, and using a linear
              readout $w_n$ of the stimuli to predict the expected reward or punishment $v_n$:

              $$ v_n = w_n^T s_n $$
            </p>

            <p>
              Over the course of the $N$ trials, the linear readout $w_n$ is updated using the prediction
              error, $r_n - v_n$ (occasionally denoted by $\delta_n$):

              $$ w_{n+1} \leftarrow w_n + \eta (r_n - v_n) s_n $$

              This learning rule is equivalent to online gradient descent under a mean-squared error loss
              between the actual reward and the expected reward:

              $$
              \begin{align*}
              L(w) &= \langle (r - v)^2 \rangle_{s} \\
              \nabla_w L(w) &= \langle  0 - 2 r s + 2 s s^T w \rangle_{s}\\
              &= 2 \langle (r - w^T s) s \rangle_{s}\\
              &\propto \langle (r - v) s \rangle_{s}
              \end{align*}
              $$
            </p>
          </div>
        </div>


        <h2 id="policy_based_rl">Policy-Based RL</h2>
        <h3 id="policy_gradient_derivation">Derive the Policy Gradient Theorem in an episodic environment.</h3>
        <p>
          Define an agent's trajectory $\tau$ as the sequence of states,
          actions and rewards it experiences: $s_1, a_1, r_1, s_2, ...$. Assuming
          the agent's policy $p_{\theta}(a|s)$ depends on parameters $\theta$, we
          can define the agent's expected return as the probability of a
          trajectory, which depends on the agent's policy, times the return of
          each trajectory, which depends only on the agent's policy through the
          trajectory:
        </p>

        <p>
          $
          \begin{align}
          \mathbb{E}_{\tau \sim p_{\theta}}[R(\tau)] = \int_{\tau} R(\tau) p_{\theta}(\tau) d\tau
          \end{align}
          $
        </p>

        <p>
          An agent that seeks to maximize its return can follow the gradient of
          its expected return with respect to its policy's parameters:
        </p>

        <p>
          $
          \begin{align}
          \nabla_{\theta} \mathbb{E}_{\tau \sim p_{\theta}}[R(\tau)] &= \nabla_{\theta} \int_{\tau} R(\tau) p_{\theta}(\tau) d\tau \nonumber \\
          &= \int_{\tau} R(\tau) \nabla_{\theta} p_{\theta}(\tau) d\tau \nonumber \\
          &= \int_{\tau} R(\tau) p_{\theta}(\tau) \nabla_{\theta} \log p_{\theta}(\tau) d\tau \nonumber \\
          &= \mathbb{E}_{\tau \sim p_{\theta}}[R(\tau) \nabla_{\theta} \log p_{\theta}(\tau)]
          \end{align}
          $
        </p>

        <p>
          Assuming that the environment obeys some transition dynamics
          $p(s_{t+1} | s_t, a_t)$,
        </p>

        <h2 id="actor_critic_rl">Actor-Critic RL</h2>
        <div>
          <h3>Gradient Estimator Baselines</h3>
          <div>
            <p>

            </p>

            <h4>Constant Baseline: $b$</h4>
            <div>
            <p>
              Unbiased:
            </p>

            <p>
              $\begin{align}
              \nabla_{\theta} \mathbb{E}_{p(\tau)} [R(\tau) - b]
              &= \nabla_{\theta}  \mathbb{E}_{p(\tau)} [R(\tau)] - \nabla_{\theta} \mathbb{E}_{p(\tau)} [b]\\
              &= \nabla_{\theta} \mathbb{E}_{p(\tau)} [R(\tau)] - b \nabla_{\theta} \mathbb{E}_{p(\tau)} [1]\\
              &= \nabla_{\theta} \mathbb{E}_{p(\tau)} [R(\tau)] - b (0)\\
              &= \nabla_{\theta} \mathbb{E}_{p(\tau)} [R(\tau)]
              \end{align}$
            </p>
            </div>

            <h4>State-Dependent Baseline: $b(s_t)$</h4>
            <div>
              <p>
                Unbiased:
              </p>

              <p>
                $\begin{align}
                \nabla_{\theta} \mathbb{E}_{p(\tau)} [R(\tau) - b]
                &= \nabla_{\theta}  \mathbb{E}_{p(\tau)} [R(\tau)] - \nabla_{\theta} \mathbb{E}_{p(\tau)} [b]\\
                &= \nabla_{\theta} \mathbb{E}_{p(\tau)} [R(\tau)] - b \nabla_{\theta} \mathbb{E}_{p(\tau)} [1]\\
                &= \nabla_{\theta} \mathbb{E}_{p(\tau)} [R(\tau)] - b (0)\\
                &= \nabla_{\theta} \mathbb{E}_{p(\tau)} [R(\tau)]
                \end{align}$
              </p>
              </div>
          </div>

        </div>

      </section>
    </div>

    <!--FOOTER  -->
    <div id="footer_wrap" class="outer">
      <footer class="inner">
        <h3 id="footer_title">Rylan Schaeffer</h3>
        <p>
          I appreciate any and all feedback. If you have suggestions or feedback or want to get in touch,
          ping me at <a href="mailto:rylanschaeffer@gmail.com">rylanschaeffer@gmail.com</a>.

          If you feel like helping me survive grad school poverty or appreciate anything on this
          website, please free free to donate to me at <a href="https://www.paypal.com/paypalme2/rylanschaeffer">
          https://www.paypal.com/paypalme2/rylanschaeffer</a>.
        </p>
      </footer>
    </div>

    
  </body>
</html>
