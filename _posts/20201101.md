---
layout: post
title:  The Idea Machine 
tags: idea-machine 2020 MIT Harvard
---

## Learning at Harvard and MIT in the Age of Artificial Intelligence

### Monday

Total work time: 

Started the morning with the I lab event on risk and forecasting. The key takeaway I learned is that timing steps in value is critical, especially when raising money. Mismatching timing is how founders are forced out

I then tested my new question for distributional RL: does learning the distributions separately yield a benefit? The idea is to use separate networks to study whether shared parameters drives the benefit or distributional learning. Currently I'm using C51 and QRDQN. Hopefully I'll have preliminary results tomorrow.

I then worked on RADD for Ila. We're currently testing the RNN coin parameter estimation as the first task. It's now implemented and I confirmed it learns a low, 1D representation. This is unexpected. Two variables should be required: the running average, and the number of samples. Curious to see how the network solves the task with 1 dimension.

For 6.438, I had a study session with Devin, Aviv and Anna. Beforehand, I tried to catch up on belief propagation. Today we focused on junction trees. Definitely need to review more.

For my amygdala fear engram project, I worked on fixing a number of small bugs. I need to get visualize how the cluster parameters change between trials to better understand how the model behaves in this regard.

I then ran 5 miles. I found a new radio station I liked. Yggdrasil by brothers of metal is currently my favorite song.

V reached back out. I'm so happy when she shows an initiative in talking to me. I wish she did more frequently.

Vincent and I created the second iteration of our market research survey and sent it out out. Judging based on people's responses, it was much better phrased. Market research isn't just about collecting data - it's about learning how to communicate to your users what value you bring


### Tuesday

Total work time: 

6.438 lecture this morning. We introduced variational inference in graphical models. Can't remember what else. How sad. Worked on the pset later with Ben. Need to catch up badly. Also reviewed notes and asked What is the difference between T steps of loopy BP on a graph and message passing on a depth T computation tree formed from the same graph?

Then had an IBL theory working group meeting to practice our presentation tomorrow. I discovered that Tatiana Engel is planning on working on a project similar to RADD, so I reached out to arrange a meeting to learn more.

Had an interview with OpenAI. Questions were

[10/27, 2:02 PM] Rylan Schaeffer: 1 (coding challenge): write a function that takes a root directory of a filesystem and returns a list of lists, where each list contains file paths with duplicate contents
[10/27, 2:02 PM] Rylan Schaeffer: 2: what is a commonly used loss for training classification? why do we prefer cross entropy over KL?
[10/27, 2:02 PM] Rylan Schaeffer: 3: what is the difference between forward mode and reverse mode KL?
[10/27, 2:03 PM] Rylan Schaeffer: 4: suppose we're doing binary classification, with a threshold at 0.5. If I know that the model correctly classifies a given point, what must the cross entropy be?
[10/27, 2:03 PM] Rylan Schaeffer: (answer: less than log 2)
[10/27, 2:05 PM] Rylan Schaeffer: 5: suppose I have N items and I sample a permutation pi: N -> N uniformly at random. I defined a "fixed point" as whether the permutation maps index i to index i
[10/27, 2:05 PM] Rylan Schaeffer: so some permutations will have 0 fixed points, others will have 1 fixed point, etc.
[10/27, 2:06 PM] Rylan Schaeffer: if I sample permutations uniformly at random, what is the expected number of fixed points?
[10/27, 2:27 PM] Rylan Schaeffer: For this one, you can write the number of fixed points as the sum of N indicator functions
[10/27, 2:27 PM] Rylan Schaeffer: I.e. pi(i)->i for all i
[10/27, 2:28 PM] Rylan Schaeffer: Each indicator has probability of 1/N as being true
[10/27, 2:28 PM] Rylan Schaeffer: But the linearity of expectation, we have N terms summed together, each with value 1/N
[10/27, 2:28 PM] Rylan Schaeffer: So the expected number of fixed points is N/N = 1
[10/27, 2:06 PM] Rylan Schaeffer: 6: discussion of generative models and autoregressive models

Took an afternoon break on the couch. Again.

I made solid progress on two of my three projects.

On Ila's, I refactored my code to enable different analysis and plotting per environment. PCA plot is wrong, or PCA is the wrong metric to use. Currently testing on estimating the parameter of a coin

On Nimrod, I realized our Bayesian non parametric model can easily be realized in a much simpler circuit than the one we were previously playing with. I implemented it. Tomorrow I will start looking into stochastic processes for excitability

On Cengiz, I did nothing.

I exercised at home, focusing on pull ups and push ups. Need to do more daily

No work on Hilbert

Worked tonight with Nate Martinez. He helped me focus

### Wednesday

Total work time: 

### Thursday

Total work time: 

Started today with 6.438 lecture. 5 pages of notes on tree-structured approximations leading up to the Bethe variational approximation. Exhausting.

I then had a 90 minute meeting with Ila, investigating the IBL mouse data and how well RADD works on the coin parameter estimation task.

I spent most of the day working on my project for Cengiz. I finished the Saxe like derivation and showed that the distributional student learns slower. I also emailed Pablo to ask for clarification on the paper they published.

On Hilbert today, Vincent and I iterated again on the survey, reposted it. We set up meetings with more advisors. Tomorrow we'll reply to Ralph Hexter, who generously offered to introduce us to some pretty heavy hitters. We also need to iterate on our business proposal and pitch

I went swimming

Tonight I chatted with Jonathan Mitchell, a former ASUCD senator.

Last night, I graded my AM207 students and found two who cheated (at least one copied off the other).

I also helped Mom prepare to interview a candidate tomorrow by giving her interview questions from my OpenAI interview.



### Friday

Total work time: 

### Saturday

Total work time: 

### Sunday

Total work time: 


Spent the whole day working on my Nimrod engram project and my 6.438 pset.

Went running.

Asked Hannah Lawrence for her Knight Hennessy Scholars application but she didn't seem keen.